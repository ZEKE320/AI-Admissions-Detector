{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22406ef1",
   "metadata": {},
   "source": [
    "# AI/Human Text Classification\n",
    "\n",
    "This project focuses on developing models to distinguish between AI-generated and human-written texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a49e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/ai_thesis_detector/submodules/AI-Admissions-Detector/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import re\n",
    "import string\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from joblib import dump\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertModel,\n",
    ")\n",
    "from transformers.models.bert.tokenization_bert import BertTokenizer\n",
    "from transformers.models.distilbert.modeling_distilbert import DistilBertModel\n",
    "from transformers.models.distilbert.tokenization_distilbert import DistilBertTokenizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23e4c8c",
   "metadata": {},
   "source": [
    "## Sample Texts\n",
    "\n",
    "Below are examples of texts used in our analysis, including AI-generated content and a mix of human and AI-assisted writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513dd623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "test_text_AI = \"\"\"During my undergraduate studies, I gained a solid foundation in programming languages such as Python and R, as well as experience working with SQL databases. I also had the opportunity to apply these skills in a real-world setting during an internship as a data analyst at a healthcare company. In this role, I was responsible for collecting, cleaning, and analyzing large datasets to provide insights into patient outcomes and healthcare costs.\n",
    "Through the Master's in Data Science program at Fordham University, I aim to further develop my expertise in data science and analytics, with a focus on machine learning and predictive modeling. I am particularly interested in courses that cover topics such as deep learning, natural language processing, and data visualization. I am confident that this program will provide me with the skills and knowledge necessary to make valuable contributions to the field of data science.\n",
    "Furthermore, I am impressed with the collaborative and interdisciplinary nature of the program, and I am excited about the opportunity to work with fellow students and faculty members from diverse backgrounds and fields. I am also attracted to the program's emphasis on practical, hands-on learning, which I believe will prepare me well for a career in data science.\n",
    "Thank you for considering my application. I am excited about the prospect of joining the Fordham University community and contributing to the vibrant academic and research environment of the Master's in Data Science program.\n",
    "Sincerely,\"\"\"\n",
    "\n",
    "test_text_mixed_Human_AI = \"\"\"Though I was barely exposed to programming while doing my B.A. in Economics, my eagerness to face new challenges led me to learn many computer languages like R and SQL. Ultimately, I decided to pursue a master's in Data Science at Fordham University, where I am fortunate to hold a Graduate Assistant position in Fordham's Computer and Information Sciences (CIS) department. Some projects have exposed me to the undertakings of high-level research and the massive amount of objectives and tasks necessary to transform a research proposal into a ranked article publication. But, most importantly, I have comprehended the role researchers play in advancing the computer science field and the gratification that yields, having contributed to it. As a result, I am driven to pursue a Ph.D. in Computer Science at NYU Tandon School of Engineering to broaden my knowledge and become an established researcher.\n",
    "In addition to my academic pursuits, I have also gained valuable experience through internships and extracurricular activities. During my undergraduate years, I interned at a non-profit organization where I assisted in developing a database management system. This experience allowed me to apply my programming skills in a real-world setting and reinforced my interest in pursuing a career in technology.\n",
    "Furthermore, I have been an active member of the Computer Science Club, where I have participated in various coding competitions and hackathons. These experiences have not only honed my technical skills but also taught me the importance of teamwork and collaboration in solving complex problems. I have also volunteered as a tutor, helping students with programming and data analysis. This experience has allowed me to share my knowledge with others and has reinforced my desire to pursue a career in academia.\n",
    "As I embark on the next stage of my academic journey, I am eager to continue building on my experiences and knowledge. I am confident that pursuing a Ph.D. in Computer Science at NYU Tandon School of Engineering will provide me with the tools and resources necessary to achieve my academic and professional goals. I look forward to contributing to the vibrant research community at NYU and making meaningful contributions to the field of Computer Science.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02e3f6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "We define a preprocessing function to clean and format text for classification, ensuring consistency by removing unnecessary characters and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a5ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text: str) -> str:\n",
    "    \"\"\"This function takes a string as input and returns a formatted version of the string.\n",
    "\n",
    "    The function replaces specific substrings in the input string with empty strings,\n",
    "    converts the string to lowercase, removes any leading or trailing whitespace,\n",
    "    and removes any punctuation from the string.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    text = \" \".join(\n",
    "        [token.text for token in doc if token.ent_type_ not in [\"PERSON\", \"DATE\"]]\n",
    "    )\n",
    "\n",
    "    pattern1 = r\"f\\d+\"\n",
    "    pattern2 = r\"\\b[A-Za-z]+\\d+\\b\"\n",
    "    pattern3 = r\"\\[(.*?)\\]\"\n",
    "\n",
    "    text = re.sub(pattern1, \"\", text)\n",
    "    text = re.sub(pattern2, \"\", text)\n",
    "    text = re.sub(pattern3, \"\", text)\n",
    "\n",
    "    return (\n",
    "        text.replace(\"REDACTED\", \"\")\n",
    "        .lower()\n",
    "        .replace(\"  \", \" \")\n",
    "        .replace(\"[Name]\", \"\")\n",
    "        .replace(\"[your name]\", \"\")\n",
    "        .replace(\"\\n your name\", \"\")\n",
    "        .replace(\"dear admissions committee,\", \"\")\n",
    "        .replace(\"sincerely,\", \"\")\n",
    "        .replace(\"[university's name]\", \"fordham\")\n",
    "        .replace(\"dear sir/madam,\", \"\")\n",
    "        .replace(\"â€“ statement of intent  \", \"\")\n",
    "        .replace(\n",
    "            \"program: master of science in data analytics  name of applicant:    \", \"\"\n",
    "        )\n",
    "        .replace(\"data analytics\", \"data science\")\n",
    "        .replace(\"| \\u200b\", \"\")\n",
    "        .replace(\"m.s. in data science at lincoln center  \", \"\")\n",
    "        .translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        .strip()\n",
    "        .lstrip()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca88de6",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In this section, we load and preprocess the data for our analysis.\n",
    "\n",
    "### Data Loading and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee19c57",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/capstoneresearchds/df_real.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_HumanGenerated \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/capstoneresearchds/df_real.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df_AIGeneratedSOI \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/capstoneresearchds/GeneratedSOIs.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m df_AIGeneratedLOR \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/capstoneresearchds/Generated_LORs.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/ai_thesis_detector/submodules/AI-Admissions-Detector/.venv/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ai_thesis_detector/submodules/AI-Admissions-Detector/.venv/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ai_thesis_detector/submodules/AI-Admissions-Detector/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ai_thesis_detector/submodules/AI-Admissions-Detector/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/workspaces/ai_thesis_detector/submodules/AI-Admissions-Detector/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ai_thesis_detector/submodules/AI-Admissions-Detector/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/workspaces/ai_thesis_detector/submodules/AI-Admissions-Detector/.venv/lib/python3.9/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/capstoneresearchds/df_real.csv'"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "df_HumanGenerated = pd.read_csv(\n",
    "    \"/kaggle/input/capstoneresearchds/df_real.csv\", dtype=str\n",
    ")\n",
    "df_AIGeneratedSOI = pd.read_csv(\n",
    "    \"/kaggle/input/capstoneresearchds/GeneratedSOIs.csv\", dtype=str\n",
    ")\n",
    "df_AIGeneratedLOR = pd.read_csv(\n",
    "    \"/kaggle/input/capstoneresearchds/Generated_LORs.csv\", dtype=str\n",
    ")\n",
    "\n",
    "# Label data: {Human:0, AI: 1}\n",
    "df_HumanGenerated[\"Target\"] = 0\n",
    "df_AIGeneratedSOI[\"Target\"] = 1\n",
    "df_AIGeneratedLOR[\"Target\"] = 1\n",
    "df_AIGeneratedSOI[\"TypeDoc\"] = \"SOI\"\n",
    "df_AIGeneratedLOR[\"TypeDoc\"] = \"LOR\"\n",
    "df_AIGeneratedSOI.rename(\n",
    "    {df_AIGeneratedSOI.columns[0]: \"ID\", df_AIGeneratedSOI.columns[2]: \"Text\"},\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "df_AIGeneratedSOI[\"ID\"] = df_AIGeneratedSOI.ID.map(lambda x: \"0\" + str(x))\n",
    "df_AIGeneratedLOR.rename({df_AIGeneratedLOR.columns[0]: \"Text\"}, axis=1, inplace=True)\n",
    "\n",
    "# Get only relevant attributes from the original datasources\n",
    "cols = [\"ID\", \"Text\", \"TypeDoc\", \"Target\"]\n",
    "df_HumanGenerated = df_HumanGenerated[cols]\n",
    "df_AIGeneratedSOI = df_AIGeneratedSOI[cols]\n",
    "\n",
    "# Union both human-written and AI generated datasets and shuffle\n",
    "df = pd.concat([df_HumanGenerated.dropna(axis=0), df_AIGeneratedSOI])[\n",
    "    [\"Text\", \"TypeDoc\", \"Target\"]\n",
    "]\n",
    "df = pd.concat([df, df_AIGeneratedLOR])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df.sample(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbaa6cf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering - For further modeling experiment\n",
    "def AvgSentence(text: str) -> float:\n",
    "    plist = text.split(\"\\n\")\n",
    "    return np.mean([p.count(\".\") for p in plist])\n",
    "\n",
    "\n",
    "df[\"CountParagraphs\"] = df.Text.map(lambda x: x.count(\"\\n\"))\n",
    "df[\"SumSentences\"] = df.Text.map(lambda x: x.count(\". \"))\n",
    "df[\"AvgSentenceByParagraphs\"] = df.Text.map(AvgSentence)\n",
    "\n",
    "df[\"Text\"] = df[\"Text\"].map(lambda x: text_cleaning(x))\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508cf3de",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "\n",
    "We divide the data into training and test sets to evaluate our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b6ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(\"Target\", axis=1), df.Target, train_size=0.80\n",
    ")\n",
    "train_sentences = X_train.Text.to_list()\n",
    "test_sentences = X_test.Text.to_list()\n",
    "\n",
    "test_sentences_LOR_ids = X_test[X_test[\"TypeDoc\"] == \"LOR\"].index\n",
    "test_sentences_SOI_ids = X_test[X_test[\"TypeDoc\"] == \"SOI\"].index\n",
    "test_sentences_LOR = X_test.loc[test_sentences_LOR_ids].Text.to_list()\n",
    "y_LORs = y_test.loc[test_sentences_LOR_ids]\n",
    "test_sentences_SOI = X_test.loc[test_sentences_SOI_ids].Text.to_list()\n",
    "y_SOIs = y_test.loc[test_sentences_SOI_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb20d1",
   "metadata": {},
   "source": [
    "## Baseline Models\n",
    "\n",
    "We start with baseline models using traditional machine learning techniques, such as logistic regression and naive Bayes, combined with TF-IDF vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f597b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression + TF-IDF Pipeline\n",
    "model_lr = Pipeline([(\"tf-idf\", TfidfVectorizer()), (\"clf\", LogisticRegression())])\n",
    "model_lr.fit(X=train_sentences, y=y_train)\n",
    "\n",
    "## Naive Bayes + TF-IDF Pipeline\n",
    "model_nb = Pipeline([(\"tf-idf\", TfidfVectorizer()), (\"clf\", MultinomialNB())])\n",
    "model_nb.fit(X=train_sentences, y=y_train)\n",
    "\n",
    "print(\"TF-IDF + LR\")\n",
    "print(f\"Accuracy: {model_lr.score(test_sentences, y_test)}\\n\")\n",
    "print(classification_report(y_test, model_lr.predict(test_sentences)), \"\\n\")\n",
    "\n",
    "print(\"TF-IDF + NB\")\n",
    "print(f\"Accuracy: {model_nb.score(test_sentences, y_test)}\\n\")\n",
    "print(classification_report(y_test, model_nb.predict(test_sentences)))\n",
    "\n",
    "# Save baseline models\n",
    "# dump(model_lr, \"baseline_model_lr.joblib\");\n",
    "# dump(model_nb, \"baseline_model_nb.joblib\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c7e5d",
   "metadata": {},
   "source": [
    "## Transformer-based Models\n",
    "\n",
    "We then explore more advanced models based on Transformers.\n",
    "\n",
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216237a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Transformers-based models\n",
    "\n",
    "\n",
    "# earlystopping callback\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 2, delta: float = 0.000001) -> None:\n",
    "        self.patience: int = patience\n",
    "        self.counter: int = 0\n",
    "        self.best_score: Optional[float] = None\n",
    "        self.delta: float = delta\n",
    "\n",
    "    def __call__(self, val_loss: float, model: nn.Module) -> bool:\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "    def save_checkpoint(self, val_loss: float, model: nn.Module) -> None:\n",
    "        torch.save(model.state_dict(), \"checkpoint.pt\")\n",
    "        print(\n",
    "            f\"Validation loss decreased ({self.best_score:.6f} --> {val_loss:.6f}). Saving model ...\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ccacf3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### DistilBERT Model Implementation\n",
    "\n",
    "Here, we implement DistilBERT, a streamlined version of the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b79d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DistilBert\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings: dict[str, list[int]], labels: np.ndarray) -> None:\n",
    "        self.encodings: dict[str, list[int]] = encodings\n",
    "        self.labels: np.ndarray = labels\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict[str, torch.Tensor]:\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# Encode the text data\n",
    "train_encodings = tokenizer(\n",
    "    train_sentences, padding=\"max_length\", max_length=512, truncation=True\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    test_sentences, padding=\"max_length\", max_length=512, truncation=True\n",
    ")\n",
    "\n",
    "# Include additional features\n",
    "columns = X_train.drop([\"Text\"], axis=1).columns.to_list()\n",
    "\n",
    "for column in columns:\n",
    "    train_encodings[column] = X_train[column].to_list()\n",
    "    val_encodings[column] = X_test[column].to_list()\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, y_train.values)\n",
    "val_dataset = TextDataset(val_encodings, y_test.values)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class TransformerBasedModelDistilBert(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(TransformerBasedModelDistilBert, self).__init__()\n",
    "        self.bert: DistilBertModel = DistilBertModel.from_pretrained(\n",
    "            \"distilbert-base-uncased\"\n",
    "        )\n",
    "        self.dropout: nn.Dropout = nn.Dropout(0.55)\n",
    "        self.fc: nn.Linear = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        input_shape = input_ids.size()\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape, device=device)\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc29aa",
   "metadata": {},
   "source": [
    "### DistilBERT Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7471f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerBasedModelDistilBert().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_total = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        train_loss += loss\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        correct = 0\n",
    "        test_loss = 0\n",
    "        total = 0\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            test_loss += loss_fn(logits, labels)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    early_stop = early_stopping(test_loss / total, model)\n",
    "\n",
    "    if early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1} out of {num_epochs} | Train Loss: {train_loss / train_total:.6f} | Test Accuracy: {correct / total:.6f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6636df",
   "metadata": {},
   "source": [
    "### DistilBERT Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20047d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### best weights\n",
    "model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
    "\n",
    "\n",
    "### Save as HuggingFace Model\n",
    "class MyConfigDistil(PretrainedConfig):\n",
    "    model_type: str = \"distilbert\"\n",
    "\n",
    "    def __init__(self, final_dropout: float = 0.55, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.final_dropout: float = final_dropout\n",
    "\n",
    "\n",
    "class MyHFModel_DistilBased(PreTrainedModel):\n",
    "    config_class: type[MyConfigDistil] = MyConfigDistil\n",
    "\n",
    "    def __init__(self, config: MyConfigDistil) -> None:\n",
    "        super().__init__(config)\n",
    "        self.config: MyConfigDistil = config\n",
    "        self.model: nn.Module = model\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        input_shape = input_ids.size()\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape)\n",
    "\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "config = MyConfigDistil(0.55)\n",
    "Custom_HF_Model = MyHFModel_DistilBased(config)\n",
    "\n",
    "Custom_HF_Model.save_pretrained(\"HF_DistilBertBasedModelAppDocs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce82acca",
   "metadata": {},
   "source": [
    "### BERT Model Implementation\n",
    "\n",
    "Next, we implement the standard BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1bdf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Bert\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Encode the text data\n",
    "train_encodings = tokenizer(train_sentences, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(test_sentences, truncation=True, padding=True)\n",
    "\n",
    "# Include additional features\n",
    "columns = X_train.drop([\"Text\"], axis=1).columns.to_list()\n",
    "\n",
    "for column in columns:\n",
    "    train_encodings[column] = X_train[column].to_list()\n",
    "    val_encodings[column] = X_test[column].to_list()\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, y_train.values)\n",
    "val_dataset = TextDataset(val_encodings, y_test.values)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class TransformerBasedModelBert(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(TransformerBasedModelBert, self).__init__()\n",
    "        self.bert: BertModel = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout: nn.Dropout = nn.Dropout(0.55)\n",
    "        self.fc: nn.Linear = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        input_shape = input_ids.size()\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape, device=device)\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6399aee",
   "metadata": {},
   "source": [
    "### BERT Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerBasedModelBert().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_total = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        train_loss += loss\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        correct = 0\n",
    "        test_loss = 0\n",
    "        total = 0\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            test_loss += loss_fn(logits, labels)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    early_stop = early_stopping(test_loss / total, model)\n",
    "\n",
    "    if early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1} out of {num_epochs} | Train Loss: {train_loss / train_total:.6f} | Test Accuracy: {correct / total:.6f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1495ed64",
   "metadata": {},
   "source": [
    "### BERT Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0df146",
   "metadata": {},
   "outputs": [],
   "source": [
    "### best weights\n",
    "model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
    "\n",
    "\n",
    "class MyConfigBert(PretrainedConfig):\n",
    "    model_type: str = \"bert\"\n",
    "\n",
    "    def __init__(self, final_dropout: float = 0.55, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.final_dropout: float = final_dropout\n",
    "\n",
    "\n",
    "class MyHFModel_BertBased(PreTrainedModel):\n",
    "    config_class: type[MyConfigBert] = MyConfigBert\n",
    "\n",
    "    def __init__(self, config: MyConfigBert) -> None:\n",
    "        super().__init__(config)\n",
    "        self.config: MyConfigBert = config\n",
    "        self.model: nn.Module = model\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        input_shape = input_ids.size()\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape, device=device)\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "config = MyConfigBert(0.55)\n",
    "Custom_HF_Model = MyHFModel_BertBased(config)\n",
    "\n",
    "Custom_HF_Model.save_pretrained(\"HF_BertBasedModelAppDocs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c623fe",
   "metadata": {},
   "source": [
    "## Extended Experiments with Additional Data\n",
    "\n",
    "We expand our training dataset by incorporating Wikipedia data to enhance model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeb97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training with Academic App docs + Wiki\n",
    "df_wiki = pd.read_csv(\"GPT-wiki-intro.csv\").sample(30000)\n",
    "originals = pd.DataFrame(df_wiki[\"wiki_intro\"]).rename({\"wiki_intro\": \"Text\"}, axis=1)\n",
    "originals[\"Target\"] = 0\n",
    "\n",
    "generated = pd.DataFrame(df_wiki[\"generated_intro\"]).rename(\n",
    "    {\"generated_intro\": \"Text\"}, axis=1\n",
    ")\n",
    "generated[\"Target\"] = 1\n",
    "\n",
    "Wiki = pd.concat([originals, generated], ignore_index=True)\n",
    "Wiki = Wiki.sample(len(Wiki))\n",
    "\n",
    "df_larger = pd.concat([df[[\"Text\", \"Target\"]], Wiki], ignore_index=True)\n",
    "df_larger = df_larger.sample(len(df_larger))\n",
    "df_larger[\"Text\"] = df_larger[\"Text\"].map(lambda x: text_cleaning(x))\n",
    "df_larger = df_larger.drop_duplicates()\n",
    "df_larger.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d026bf",
   "metadata": {},
   "source": [
    "### Baseline Models with Extended Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c554cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_larger.drop(\"Target\", axis=1), df_larger.Target, train_size=0.80\n",
    ")\n",
    "train_sentences = X_train.Text.to_list()\n",
    "test_sentences = X_test.Text.to_list()\n",
    "# Baseline models\n",
    "model_lr = Pipeline([(\"tf-idf\", TfidfVectorizer()), (\"clf\", LogisticRegression())])\n",
    "model_lr.fit(X=train_sentences, y=y_train)\n",
    "\n",
    "model_nb = Pipeline([(\"tf-idf\", TfidfVectorizer()), (\"clf\", MultinomialNB())])\n",
    "model_nb.fit(X=train_sentences, y=y_train)\n",
    "\n",
    "print(\"TF-IDF + LR\")\n",
    "print(f\"Accuracy: {model_lr.score(test_sentences, y_test)}\\n\")\n",
    "print(classification_report(y_test, model_lr.predict(test_sentences)), \"\\n\")\n",
    "\n",
    "print(\"TF-IDF + NB\")\n",
    "print(f\"Accuracy: {model_nb.score(test_sentences, y_test)}\\n\")\n",
    "print(classification_report(y_test, model_nb.predict(test_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91258d2",
   "metadata": {},
   "source": [
    "### Saving Extended Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e25293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline model\n",
    "dump(model_lr, \"baseline_model_lr2.joblib\")\n",
    "dump(model_nb, \"baseline_model_nb2.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b9781",
   "metadata": {},
   "source": [
    "### DistilBERT Model with Extended Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f511867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers-based models\n",
    "\n",
    "## DistilBert\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# Encode the text data\n",
    "train_encodings = tokenizer(\n",
    "    train_sentences, padding=\"max_length\", max_length=512, truncation=True\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    test_sentences, padding=\"max_length\", max_length=512, truncation=True\n",
    ")\n",
    "\n",
    "# Include additional features\n",
    "columns = X_train.drop([\"Text\"], axis=1).columns.to_list()\n",
    "\n",
    "for column in columns:\n",
    "    train_encodings[column] = X_train[column].to_list()\n",
    "    val_encodings[column] = X_test[column].to_list()\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, y_train.values)\n",
    "val_dataset = TextDataset(val_encodings, y_test.values)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = TransformerBasedModelDistilBert().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_total = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        train_loss += loss\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        correct = 0\n",
    "        test_loss = 0\n",
    "        total = 0\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            test_loss += loss_fn(logits, labels)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    early_stop = early_stopping(test_loss / total, model)\n",
    "\n",
    "    if early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1} out of {num_epochs} | Train Loss: {train_loss / train_total:.6f} | Test Accuracy: {correct / total:.6f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c962a3",
   "metadata": {},
   "source": [
    "### Saving Extended DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### best weights\n",
    "model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
    "\n",
    "### Save as HuggingFace Model\n",
    "config = MyConfigDistil(0.55)\n",
    "Custom_HF_Model = MyHFModel_DistilBased(config)\n",
    "\n",
    "Custom_HF_Model.save_pretrained(\"HF_DistilBertBasedModelAppDocs2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653a2953",
   "metadata": {},
   "source": [
    "### BERT Model with Extended Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Bert\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Encode the text data\n",
    "train_encodings = tokenizer(train_sentences, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(test_sentences, truncation=True, padding=True)\n",
    "\n",
    "# Include additional features\n",
    "columns = X_train.drop([\"Text\"], axis=1).columns.to_list()\n",
    "\n",
    "for column in columns:\n",
    "    train_encodings[column] = X_train[column].to_list()\n",
    "    val_encodings[column] = X_test[column].to_list()\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, y_train.values)\n",
    "val_dataset = TextDataset(val_encodings, y_test.values)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = TransformerBasedModelBert().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_total = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        train_loss += loss\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        correct = 0\n",
    "        test_loss = 0\n",
    "        total = 0\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            test_loss += loss_fn(logits, labels)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    early_stop = early_stopping(test_loss / total, model)\n",
    "\n",
    "    if early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1} out of {num_epochs} | Train Loss: {train_loss / train_total:.6f} | Test Accuracy: {correct / total:.6f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb1580",
   "metadata": {},
   "source": [
    "### Saving Extended BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354bf62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### best weights\n",
    "model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
    "\n",
    "config = MyConfigBert(0.55)\n",
    "Custom_HF_Model = MyHFModel_BertBased(config)\n",
    "\n",
    "Custom_HF_Model.save_pretrained(\"HF_BertBasedModelAppDocs2\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
